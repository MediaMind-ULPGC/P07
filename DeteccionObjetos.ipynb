{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96af097-b0c3-4a7c-bd6d-7ff540317b85",
   "metadata": {},
   "source": [
    "# Detección de Objetos\n",
    "- COCO Dataset\n",
    "- Faster R-CNN model\n",
    "\n",
    "<b>Fuentes:</b> \n",
    "1. [Pytorch tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "2. [PyTorch object detection with pre-trained networks](https://pyimagesearch.com/2021/08/02/pytorch-object-detection-with-pre-trained-networks/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "833fb5fe-f8a5-4111-b886-19f795b07d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0340d288-f343-485e-a53e-8c8172add893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 91\n"
     ]
    }
   ],
   "source": [
    "# Definición de constantes \n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# esto es para que funcione matplotlib \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "IMAGE_SIZE = 256\n",
    "MODELS_DIR = 'models'\n",
    "CLASSES = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', \n",
    "           'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "           'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', \n",
    "           'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', \n",
    "           'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', \n",
    "           'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', \n",
    "           'snowboard', 'sports ball', 'kite', 'baseball bat', \n",
    "           'baseball glove', 'skateboard', 'surfboard', 'tennis racket', \n",
    "           'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', \n",
    "           'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', \n",
    "           'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "           'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', \n",
    "           'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "           'cell phone', 'microwave', 'oven', 'toaster', 'sink', \n",
    "           'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', \n",
    "           'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "print('Num classes:', len(CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92683a-819e-432e-8307-6b871c716658",
   "metadata": {},
   "source": [
    "## Función para dibujar los *bounding boxes* en la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99fb707f-4a6f-4713-8ef4-d5570473404e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dibujamos los objetos detectados en la imagen\n",
    "import cv2 as cv\n",
    "\n",
    "def draw_bbox(detections,image, means=None):\n",
    "\n",
    "    for i in range(len(detections[\"boxes\"])):\n",
    "\n",
    "        # seleccionamos objetos con un confidence suficientemente grande\n",
    "        confidence = detections[\"scores\"][i]\n",
    "\n",
    "        if confidence > 0.8:\n",
    "\n",
    "            # extraemos el bounding box\n",
    "            box = detections[\"boxes\"][i].detach().cpu().numpy()\n",
    "            x0, y0, x1, y1 = box.astype(\"int\")\n",
    "\n",
    "            # extraemos la etiqueta\n",
    "            idx = int(detections[\"labels\"][i])\n",
    "            label = CLASSES[idx]\n",
    "\n",
    "            # dibujamos el bounding box y la etiqueta en la imagen\n",
    "            cv.rectangle(image, (x0, y0), (x1, y1), COLORS[idx], 2)\n",
    "            y = y0 - 15 if y0 - 15 > 15 else y0 + 15\n",
    "            text = f\"{label} - {confidence:.2f}\"\n",
    "            cv.putText(image, text, (x0, y), cv.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "            \n",
    "            # mostramos el número de objetos\n",
    "            if means != None:\n",
    "                text = f\"Objetos: {len(means)}\"\n",
    "                cv.putText(image, text, (20,20), cv.FONT_HERSHEY_SIMPLEX, 1,(255,255,255), 2)\n",
    "            \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec126f-d097-455e-ba83-3c3920384f82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Detección de objetos en imágenes con Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764d10c-6fd0-48c6-94db-12ee74225b89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'PoseModel' on <module 'ultralytics.nn.tasks' from 'c:\\\\Users\\\\gerar\\\\anaconda3\\\\envs\\\\aa1\\\\lib\\\\site-packages\\\\ultralytics\\\\nn\\\\tasks.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#model = detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolo11n-pose.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# load the image from disk\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:55\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, type)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m {}  \u001b[38;5;66;03m# overrides for trainer object\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Load or create new YOLO model\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:83\u001b[0m, in \u001b[0;36mYOLO._load\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    > Initializes a new model and infers the task type from the model head.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m        weights (str): model checkpoint to be loaded\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path \u001b[38;5;241m=\u001b[39m weights\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\ultralytics\\nn\\tasks.py:341\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;66;03m# Loads a single model weights\u001b[39;00m\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_download\n\u001b[1;32m--> 341\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattempt_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m    342\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CONFIG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m'\u001b[39m]}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torch\\serialization.py:1415\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'PoseModel' on <module 'ultralytics.nn.tasks' from 'c:\\\\Users\\\\gerar\\\\anaconda3\\\\envs\\\\aa1\\\\lib\\\\site-packages\\\\ultralytics\\\\nn\\\\tasks.py'>"
     ]
    }
   ],
   "source": [
    "# Detección de objetos en la imagen con Faster R-CNN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.models import detection\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "\n",
    "#model = detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model = YOLO('yolo11n-pose.pt').to(device)\n",
    "model.eval()\n",
    "\n",
    "# load the image from disk\n",
    "image_name = 'images/prueba3.jpg'\n",
    "image = cv.imread(image_name)\n",
    "\n",
    "# convertimos a RGB y ponemos en formato pytorch\n",
    "orig = image.copy()\n",
    "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "image = image.transpose((2, 0, 1))\n",
    "\n",
    "# añadimos dimensión para el batch y normalizamos\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0\n",
    "image = torch.FloatTensor(image)\n",
    "\n",
    "# ejecutamos el modelo con la imagen\n",
    "image = image.to(device)\n",
    "detections = model(image)[0]\n",
    "\n",
    "# dibujamos las cajas con las etiquetas y scores\n",
    "output = draw_bbox(detections, orig)\n",
    "\n",
    "# mostramos la salida\n",
    "cv.imshow('Output', output)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9dc6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detección de objetos en la imagen con Faster R-CNN\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2 as cv\n",
    "from torchvision.models import detection\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "\n",
    "#model = detection.fasterrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model = YOLO('./yolov8x-pose.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c754ddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 12 persons, 1374.3ms\n",
      "Speed: 6.0ms preprocess, 1374.3ms inference, 3.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "ultralytics.engine.results.Keypoints object with attributes:\n",
      "\n",
      "conf: tensor([[0.9727, 0.9708, 0.6449, 0.9646, 0.0915, 0.9972, 0.9973, 0.9893, 0.9863, 0.9870, 0.9826, 0.9992, 0.9992, 0.9983, 0.9984, 0.9884, 0.9895],\n",
      "        [0.9937, 0.9876, 0.9729, 0.8931, 0.6742, 0.9974, 0.9971, 0.9816, 0.9792, 0.9741, 0.9766, 0.9964, 0.9963, 0.9804, 0.9812, 0.8737, 0.8781],\n",
      "        [0.9939, 0.9948, 0.9302, 0.9720, 0.1837, 0.9981, 0.9969, 0.9935, 0.9799, 0.9930, 0.9807, 0.9992, 0.9989, 0.9986, 0.9978, 0.9881, 0.9844],\n",
      "        [0.9700, 0.9489, 0.9362, 0.7410, 0.6182, 0.9945, 0.9954, 0.9427, 0.9565, 0.9076, 0.9255, 0.9969, 0.9973, 0.9933, 0.9941, 0.9732, 0.9759],\n",
      "        [0.9727, 0.9484, 0.9484, 0.6368, 0.6501, 0.9933, 0.9845, 0.9382, 0.8111, 0.8907, 0.7769, 0.9947, 0.9920, 0.9894, 0.9825, 0.9711, 0.9568],\n",
      "        [0.9852, 0.9827, 0.9349, 0.9297, 0.4271, 0.9970, 0.9976, 0.9713, 0.9669, 0.9513, 0.9381, 0.9977, 0.9979, 0.9928, 0.9930, 0.9752, 0.9762],\n",
      "        [0.3664, 0.3745, 0.1383, 0.4401, 0.1105, 0.9889, 0.6563, 0.9786, 0.1477, 0.9310, 0.1320, 0.9971, 0.9761, 0.9966, 0.9648, 0.9893, 0.9364],\n",
      "        [0.7047, 0.3205, 0.8266, 0.0649, 0.7322, 0.2919, 0.9797, 0.0171, 0.9547, 0.0173, 0.8457, 0.6984, 0.9671, 0.5147, 0.9479, 0.3642, 0.8341],\n",
      "        [0.9495, 0.8329, 0.9503, 0.3072, 0.8642, 0.9916, 0.9904, 0.9043, 0.9194, 0.8459, 0.8720, 0.9953, 0.9950, 0.9924, 0.9924, 0.9812, 0.9806],\n",
      "        [0.1335, 0.0441, 0.1558, 0.0341, 0.2413, 0.2788, 0.7205, 0.0787, 0.7108, 0.1069, 0.5820, 0.7584, 0.8937, 0.7848, 0.9237, 0.6887, 0.8513],\n",
      "        [0.8509, 0.7366, 0.8300, 0.3447, 0.5746, 0.8607, 0.9733, 0.3543, 0.9012, 0.3555, 0.8133, 0.9426, 0.9758, 0.9291, 0.9742, 0.9038, 0.9564],\n",
      "        [0.8298, 0.5479, 0.8990, 0.1159, 0.8355, 0.8440, 0.9836, 0.1981, 0.9501, 0.2221, 0.8876, 0.9593, 0.9883, 0.9609, 0.9912, 0.9500, 0.9843]])\n",
      "data: tensor([[[4.9773e+02, 1.9620e+02, 9.7271e-01],\n",
      "         [5.0146e+02, 1.9447e+02, 9.7075e-01],\n",
      "         [4.9741e+02, 1.9429e+02, 6.4489e-01],\n",
      "         [5.0968e+02, 1.9962e+02, 9.6464e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 9.1507e-02],\n",
      "         [5.1931e+02, 2.1931e+02, 9.9724e-01],\n",
      "         [4.8956e+02, 2.1833e+02, 9.9729e-01],\n",
      "         [5.2162e+02, 2.4391e+02, 9.8931e-01],\n",
      "         [4.8420e+02, 2.4296e+02, 9.8633e-01],\n",
      "         [5.2451e+02, 2.6611e+02, 9.8703e-01],\n",
      "         [4.7869e+02, 2.6393e+02, 9.8257e-01],\n",
      "         [5.1556e+02, 2.6479e+02, 9.9915e-01],\n",
      "         [4.9619e+02, 2.6440e+02, 9.9916e-01],\n",
      "         [5.1448e+02, 3.0122e+02, 9.9831e-01],\n",
      "         [5.0050e+02, 3.0016e+02, 9.9836e-01],\n",
      "         [5.0887e+02, 3.3115e+02, 9.8837e-01],\n",
      "         [5.0251e+02, 3.3542e+02, 9.8954e-01]],\n",
      "\n",
      "        [[6.1405e+02, 1.7995e+02, 9.9369e-01],\n",
      "         [6.1748e+02, 1.7664e+02, 9.8756e-01],\n",
      "         [6.1058e+02, 1.7705e+02, 9.7286e-01],\n",
      "         [6.2326e+02, 1.7959e+02, 8.9306e-01],\n",
      "         [6.0632e+02, 1.8035e+02, 6.7425e-01],\n",
      "         [6.3468e+02, 2.0208e+02, 9.9742e-01],\n",
      "         [5.9973e+02, 2.0210e+02, 9.9711e-01],\n",
      "         [6.4307e+02, 2.2913e+02, 9.8163e-01],\n",
      "         [5.9540e+02, 2.2175e+02, 9.7923e-01],\n",
      "         [6.4802e+02, 2.5237e+02, 9.7406e-01],\n",
      "         [6.0786e+02, 2.0452e+02, 9.7657e-01],\n",
      "         [6.3335e+02, 2.5414e+02, 9.9637e-01],\n",
      "         [6.0958e+02, 2.5498e+02, 9.9631e-01],\n",
      "         [6.3836e+02, 2.9583e+02, 9.8036e-01],\n",
      "         [6.1058e+02, 2.9682e+02, 9.8118e-01],\n",
      "         [6.4373e+02, 3.3238e+02, 8.7367e-01],\n",
      "         [6.1118e+02, 3.3630e+02, 8.7807e-01]],\n",
      "\n",
      "        [[1.8566e+02, 1.2660e+02, 9.9395e-01],\n",
      "         [1.9199e+02, 1.1977e+02, 9.9479e-01],\n",
      "         [1.8060e+02, 1.1953e+02, 9.3016e-01],\n",
      "         [2.0490e+02, 1.1495e+02, 9.7199e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.8368e-01],\n",
      "         [2.2167e+02, 1.4779e+02, 9.9810e-01],\n",
      "         [1.6012e+02, 1.4532e+02, 9.9691e-01],\n",
      "         [2.3323e+02, 1.9424e+02, 9.9354e-01],\n",
      "         [1.3849e+02, 1.8447e+02, 9.7988e-01],\n",
      "         [2.0267e+02, 1.9114e+02, 9.9305e-01],\n",
      "         [1.5744e+02, 1.8797e+02, 9.8069e-01],\n",
      "         [2.0258e+02, 2.4125e+02, 9.9915e-01],\n",
      "         [1.6401e+02, 2.3948e+02, 9.9891e-01],\n",
      "         [2.0048e+02, 3.1000e+02, 9.9863e-01],\n",
      "         [1.6233e+02, 3.0660e+02, 9.9783e-01],\n",
      "         [1.9339e+02, 3.7547e+02, 9.8812e-01],\n",
      "         [1.7720e+02, 3.6135e+02, 9.8435e-01]],\n",
      "\n",
      "        [[3.6188e+02, 2.3424e+02, 9.7005e-01],\n",
      "         [3.6444e+02, 2.3151e+02, 9.4895e-01],\n",
      "         [3.5958e+02, 2.3161e+02, 9.3616e-01],\n",
      "         [3.6923e+02, 2.3184e+02, 7.4100e-01],\n",
      "         [3.5679e+02, 2.3206e+02, 6.1822e-01],\n",
      "         [3.7498e+02, 2.4528e+02, 9.9448e-01],\n",
      "         [3.5346e+02, 2.4446e+02, 9.9539e-01],\n",
      "         [3.7781e+02, 2.6025e+02, 9.4275e-01],\n",
      "         [3.4836e+02, 2.5684e+02, 9.5649e-01],\n",
      "         [3.6766e+02, 2.4791e+02, 9.0765e-01],\n",
      "         [3.5478e+02, 2.4284e+02, 9.2552e-01],\n",
      "         [3.6856e+02, 2.7841e+02, 9.9689e-01],\n",
      "         [3.5447e+02, 2.7784e+02, 9.9728e-01],\n",
      "         [3.6729e+02, 3.0550e+02, 9.9325e-01],\n",
      "         [3.5485e+02, 3.0464e+02, 9.9406e-01],\n",
      "         [3.6573e+02, 3.3123e+02, 9.7324e-01],\n",
      "         [3.5585e+02, 3.3035e+02, 9.7586e-01]],\n",
      "\n",
      "        [[2.9890e+02, 2.3180e+02, 9.7269e-01],\n",
      "         [3.0098e+02, 2.2953e+02, 9.4842e-01],\n",
      "         [2.9646e+02, 2.2982e+02, 9.4835e-01],\n",
      "         [3.0414e+02, 2.3117e+02, 6.3682e-01],\n",
      "         [2.9314e+02, 2.3174e+02, 6.5010e-01],\n",
      "         [3.0897e+02, 2.4462e+02, 9.9326e-01],\n",
      "         [2.8961e+02, 2.4328e+02, 9.8450e-01],\n",
      "         [3.1348e+02, 2.5954e+02, 9.3824e-01],\n",
      "         [2.8428e+02, 2.5168e+02, 8.1107e-01],\n",
      "         [3.0961e+02, 2.6212e+02, 8.9073e-01],\n",
      "         [2.8817e+02, 2.4438e+02, 7.7687e-01],\n",
      "         [3.0470e+02, 2.7628e+02, 9.9467e-01],\n",
      "         [2.9211e+02, 2.7575e+02, 9.9196e-01],\n",
      "         [3.0363e+02, 3.0276e+02, 9.8937e-01],\n",
      "         [2.9242e+02, 3.0228e+02, 9.8245e-01],\n",
      "         [3.0292e+02, 3.2785e+02, 9.7112e-01],\n",
      "         [2.9608e+02, 3.2705e+02, 9.5677e-01]],\n",
      "\n",
      "        [[4.2668e+02, 2.4169e+02, 9.8517e-01],\n",
      "         [4.2892e+02, 2.3991e+02, 9.8272e-01],\n",
      "         [4.2534e+02, 2.3957e+02, 9.3491e-01],\n",
      "         [4.3279e+02, 2.4124e+02, 9.2972e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.2706e-01],\n",
      "         [4.3672e+02, 2.5307e+02, 9.9704e-01],\n",
      "         [4.1897e+02, 2.5217e+02, 9.9764e-01],\n",
      "         [4.3890e+02, 2.6728e+02, 9.7133e-01],\n",
      "         [4.1429e+02, 2.6627e+02, 9.6694e-01],\n",
      "         [4.3686e+02, 2.7875e+02, 9.5134e-01],\n",
      "         [4.1125e+02, 2.7980e+02, 9.3809e-01],\n",
      "         [4.3202e+02, 2.8191e+02, 9.9767e-01],\n",
      "         [4.2084e+02, 2.8143e+02, 9.9790e-01],\n",
      "         [4.3133e+02, 3.0281e+02, 9.9278e-01],\n",
      "         [4.2137e+02, 3.0208e+02, 9.9304e-01],\n",
      "         [4.3077e+02, 3.2233e+02, 9.7515e-01],\n",
      "         [4.2444e+02, 3.2150e+02, 9.7620e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 3.6641e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.7453e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3834e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4010e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1051e-01],\n",
      "         [2.5290e+02, 2.1322e+02, 9.8893e-01],\n",
      "         [2.2239e+02, 2.1250e+02, 6.5629e-01],\n",
      "         [2.5991e+02, 2.3618e+02, 9.7862e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.4772e-01],\n",
      "         [2.5591e+02, 2.5453e+02, 9.3102e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.3204e-01],\n",
      "         [2.4426e+02, 2.6555e+02, 9.9706e-01],\n",
      "         [2.2390e+02, 2.6520e+02, 9.7607e-01],\n",
      "         [2.4263e+02, 3.0315e+02, 9.9660e-01],\n",
      "         [2.2089e+02, 3.0274e+02, 9.6475e-01],\n",
      "         [2.3929e+02, 3.3889e+02, 9.8934e-01],\n",
      "         [2.2041e+02, 3.3817e+02, 9.3638e-01]],\n",
      "\n",
      "        [[1.6856e+02, 1.3185e+02, 7.0472e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.2053e-01],\n",
      "         [1.6365e+02, 1.2693e+02, 8.2663e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 6.4922e-02],\n",
      "         [1.5488e+02, 1.2923e+02, 7.3223e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.9195e-01],\n",
      "         [1.4329e+02, 1.5897e+02, 9.7968e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7094e-02],\n",
      "         [1.2559e+02, 1.9420e+02, 9.5468e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.7251e-02],\n",
      "         [1.4837e+02, 1.9511e+02, 8.4571e-01],\n",
      "         [1.7858e+02, 2.3537e+02, 6.9845e-01],\n",
      "         [1.5231e+02, 2.3532e+02, 9.6708e-01],\n",
      "         [1.7485e+02, 2.9559e+02, 5.1472e-01],\n",
      "         [1.5618e+02, 2.9542e+02, 9.4793e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.6424e-01],\n",
      "         [1.6395e+02, 3.5326e+02, 8.3413e-01]],\n",
      "\n",
      "        [[4.5432e+02, 2.4041e+02, 9.4949e-01],\n",
      "         [4.5581e+02, 2.3860e+02, 8.3293e-01],\n",
      "         [4.5238e+02, 2.3839e+02, 9.5034e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.0716e-01],\n",
      "         [4.4805e+02, 2.3967e+02, 8.6425e-01],\n",
      "         [4.5930e+02, 2.5136e+02, 9.9158e-01],\n",
      "         [4.4309e+02, 2.5127e+02, 9.9042e-01],\n",
      "         [4.6301e+02, 2.6479e+02, 9.0432e-01],\n",
      "         [4.3927e+02, 2.6510e+02, 9.1943e-01],\n",
      "         [4.5873e+02, 2.6616e+02, 8.4588e-01],\n",
      "         [4.4399e+02, 2.6416e+02, 8.7201e-01],\n",
      "         [4.5681e+02, 2.7848e+02, 9.9526e-01],\n",
      "         [4.4567e+02, 2.7885e+02, 9.9502e-01],\n",
      "         [4.6044e+02, 3.0111e+02, 9.9243e-01],\n",
      "         [4.4705e+02, 3.0183e+02, 9.9236e-01],\n",
      "         [4.6463e+02, 3.2259e+02, 9.8125e-01],\n",
      "         [4.4736e+02, 3.2377e+02, 9.8063e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 1.3347e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 4.4088e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 1.5584e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4118e-02],\n",
      "         [0.0000e+00, 0.0000e+00, 2.4130e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.7880e-01],\n",
      "         [1.1762e+02, 2.1871e+02, 7.2046e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 7.8661e-02],\n",
      "         [1.1757e+02, 2.3716e+02, 7.1079e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.0692e-01],\n",
      "         [1.2659e+02, 2.4082e+02, 5.8198e-01],\n",
      "         [1.2328e+02, 2.5573e+02, 7.5840e-01],\n",
      "         [1.2147e+02, 2.5680e+02, 8.9369e-01],\n",
      "         [1.2583e+02, 2.9397e+02, 7.8476e-01],\n",
      "         [1.2057e+02, 2.9457e+02, 9.2372e-01],\n",
      "         [1.2286e+02, 3.2798e+02, 6.8872e-01],\n",
      "         [1.1636e+02, 3.2896e+02, 8.5134e-01]],\n",
      "\n",
      "        [[3.4409e+02, 2.4085e+02, 8.5088e-01],\n",
      "         [3.4615e+02, 2.3879e+02, 7.3664e-01],\n",
      "         [3.4206e+02, 2.3875e+02, 8.3002e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.4468e-01],\n",
      "         [3.3900e+02, 2.4013e+02, 5.7459e-01],\n",
      "         [3.5256e+02, 2.5190e+02, 8.6069e-01],\n",
      "         [3.3496e+02, 2.5190e+02, 9.7325e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5427e-01],\n",
      "         [3.3003e+02, 2.6695e+02, 9.0122e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 3.5545e-01],\n",
      "         [3.2746e+02, 2.8019e+02, 8.1331e-01],\n",
      "         [3.4948e+02, 2.8077e+02, 9.4260e-01],\n",
      "         [3.3821e+02, 2.8073e+02, 9.7579e-01],\n",
      "         [3.4898e+02, 3.0400e+02, 9.2905e-01],\n",
      "         [3.3842e+02, 3.0358e+02, 9.7421e-01],\n",
      "         [3.4831e+02, 3.2473e+02, 9.0381e-01],\n",
      "         [3.3878e+02, 3.2454e+02, 9.5635e-01]],\n",
      "\n",
      "        [[2.8131e+02, 2.4057e+02, 8.2980e-01],\n",
      "         [2.8275e+02, 2.3866e+02, 5.4785e-01],\n",
      "         [2.7918e+02, 2.3857e+02, 8.9897e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.1589e-01],\n",
      "         [2.7487e+02, 2.3956e+02, 8.3547e-01],\n",
      "         [2.8626e+02, 2.5080e+02, 8.4398e-01],\n",
      "         [2.6879e+02, 2.4971e+02, 9.8360e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 1.9815e-01],\n",
      "         [2.6210e+02, 2.6000e+02, 9.5013e-01],\n",
      "         [0.0000e+00, 0.0000e+00, 2.2211e-01],\n",
      "         [2.6572e+02, 2.6427e+02, 8.8758e-01],\n",
      "         [2.8345e+02, 2.7837e+02, 9.5926e-01],\n",
      "         [2.7235e+02, 2.7814e+02, 9.8831e-01],\n",
      "         [2.8184e+02, 3.0151e+02, 9.6086e-01],\n",
      "         [2.7238e+02, 3.0100e+02, 9.9122e-01],\n",
      "         [2.7889e+02, 3.2242e+02, 9.5005e-01],\n",
      "         [2.7094e+02, 3.2227e+02, 9.8428e-01]]])\n",
      "has_visible: True\n",
      "orig_shape: (485, 728)\n",
      "shape: torch.Size([12, 17, 3])\n",
      "xy: tensor([[[497.7255, 196.1996],\n",
      "         [501.4617, 194.4671],\n",
      "         [497.4061, 194.2859],\n",
      "         [509.6758, 199.6156],\n",
      "         [  0.0000,   0.0000],\n",
      "         [519.3077, 219.3068],\n",
      "         [489.5634, 218.3251],\n",
      "         [521.6218, 243.9067],\n",
      "         [484.2040, 242.9576],\n",
      "         [524.5084, 266.1091],\n",
      "         [478.6856, 263.9252],\n",
      "         [515.5557, 264.7924],\n",
      "         [496.1929, 264.4020],\n",
      "         [514.4753, 301.2212],\n",
      "         [500.4954, 300.1625],\n",
      "         [508.8658, 331.1513],\n",
      "         [502.5053, 335.4179]],\n",
      "\n",
      "        [[614.0511, 179.9525],\n",
      "         [617.4825, 176.6412],\n",
      "         [610.5780, 177.0499],\n",
      "         [623.2566, 179.5876],\n",
      "         [606.3162, 180.3505],\n",
      "         [634.6763, 202.0787],\n",
      "         [599.7270, 202.1004],\n",
      "         [643.0682, 229.1278],\n",
      "         [595.3976, 221.7520],\n",
      "         [648.0236, 252.3667],\n",
      "         [607.8599, 204.5235],\n",
      "         [633.3519, 254.1356],\n",
      "         [609.5775, 254.9815],\n",
      "         [638.3589, 295.8298],\n",
      "         [610.5776, 296.8183],\n",
      "         [643.7286, 332.3771],\n",
      "         [611.1813, 336.2982]],\n",
      "\n",
      "        [[185.6581, 126.6002],\n",
      "         [191.9932, 119.7719],\n",
      "         [180.5950, 119.5256],\n",
      "         [204.8981, 114.9530],\n",
      "         [  0.0000,   0.0000],\n",
      "         [221.6684, 147.7933],\n",
      "         [160.1229, 145.3247],\n",
      "         [233.2308, 194.2450],\n",
      "         [138.4885, 184.4664],\n",
      "         [202.6729, 191.1434],\n",
      "         [157.4438, 187.9653],\n",
      "         [202.5756, 241.2536],\n",
      "         [164.0086, 239.4829],\n",
      "         [200.4778, 309.9969],\n",
      "         [162.3341, 306.5982],\n",
      "         [193.3943, 375.4720],\n",
      "         [177.2040, 361.3495]],\n",
      "\n",
      "        [[361.8752, 234.2394],\n",
      "         [364.4424, 231.5065],\n",
      "         [359.5779, 231.6053],\n",
      "         [369.2287, 231.8364],\n",
      "         [356.7865, 232.0554],\n",
      "         [374.9754, 245.2839],\n",
      "         [353.4608, 244.4623],\n",
      "         [377.8073, 260.2549],\n",
      "         [348.3618, 256.8356],\n",
      "         [367.6609, 247.9079],\n",
      "         [354.7791, 242.8355],\n",
      "         [368.5634, 278.4140],\n",
      "         [354.4652, 277.8434],\n",
      "         [367.2866, 305.5030],\n",
      "         [354.8469, 304.6448],\n",
      "         [365.7337, 331.2307],\n",
      "         [355.8460, 330.3513]],\n",
      "\n",
      "        [[298.8956, 231.8044],\n",
      "         [300.9762, 229.5298],\n",
      "         [296.4590, 229.8213],\n",
      "         [304.1379, 231.1698],\n",
      "         [293.1423, 231.7400],\n",
      "         [308.9743, 244.6245],\n",
      "         [289.6121, 243.2826],\n",
      "         [313.4801, 259.5435],\n",
      "         [284.2820, 251.6805],\n",
      "         [309.6137, 262.1163],\n",
      "         [288.1693, 244.3839],\n",
      "         [304.7004, 276.2796],\n",
      "         [292.1141, 275.7499],\n",
      "         [303.6313, 302.7625],\n",
      "         [292.4168, 302.2779],\n",
      "         [302.9242, 327.8461],\n",
      "         [296.0815, 327.0467]],\n",
      "\n",
      "        [[426.6826, 241.6854],\n",
      "         [428.9188, 239.9132],\n",
      "         [425.3439, 239.5658],\n",
      "         [432.7912, 241.2352],\n",
      "         [  0.0000,   0.0000],\n",
      "         [436.7190, 253.0748],\n",
      "         [418.9681, 252.1706],\n",
      "         [438.9012, 267.2766],\n",
      "         [414.2931, 266.2709],\n",
      "         [436.8640, 278.7461],\n",
      "         [411.2521, 279.7992],\n",
      "         [432.0246, 281.9096],\n",
      "         [420.8428, 281.4350],\n",
      "         [431.3293, 302.8104],\n",
      "         [421.3713, 302.0850],\n",
      "         [430.7695, 322.3293],\n",
      "         [424.4431, 321.4983]],\n",
      "\n",
      "        [[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [252.8998, 213.2214],\n",
      "         [222.3898, 212.5025],\n",
      "         [259.9052, 236.1827],\n",
      "         [  0.0000,   0.0000],\n",
      "         [255.9095, 254.5310],\n",
      "         [  0.0000,   0.0000],\n",
      "         [244.2634, 265.5535],\n",
      "         [223.8988, 265.1995],\n",
      "         [242.6329, 303.1461],\n",
      "         [220.8873, 302.7434],\n",
      "         [239.2856, 338.8945],\n",
      "         [220.4095, 338.1728]],\n",
      "\n",
      "        [[168.5572, 131.8526],\n",
      "         [  0.0000,   0.0000],\n",
      "         [163.6539, 126.9322],\n",
      "         [  0.0000,   0.0000],\n",
      "         [154.8818, 129.2282],\n",
      "         [  0.0000,   0.0000],\n",
      "         [143.2928, 158.9677],\n",
      "         [  0.0000,   0.0000],\n",
      "         [125.5873, 194.2029],\n",
      "         [  0.0000,   0.0000],\n",
      "         [148.3651, 195.1076],\n",
      "         [178.5845, 235.3701],\n",
      "         [152.3075, 235.3211],\n",
      "         [174.8518, 295.5890],\n",
      "         [156.1849, 295.4193],\n",
      "         [  0.0000,   0.0000],\n",
      "         [163.9467, 353.2562]],\n",
      "\n",
      "        [[454.3210, 240.4103],\n",
      "         [455.8101, 238.6004],\n",
      "         [452.3809, 238.3922],\n",
      "         [  0.0000,   0.0000],\n",
      "         [448.0543, 239.6655],\n",
      "         [459.2980, 251.3594],\n",
      "         [443.0940, 251.2686],\n",
      "         [463.0114, 264.7910],\n",
      "         [439.2749, 265.1046],\n",
      "         [458.7257, 266.1626],\n",
      "         [443.9893, 264.1620],\n",
      "         [456.8066, 278.4801],\n",
      "         [445.6710, 278.8468],\n",
      "         [460.4370, 301.1056],\n",
      "         [447.0501, 301.8300],\n",
      "         [464.6338, 322.5949],\n",
      "         [447.3644, 323.7712]],\n",
      "\n",
      "        [[  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000],\n",
      "         [117.6248, 218.7129],\n",
      "         [  0.0000,   0.0000],\n",
      "         [117.5681, 237.1622],\n",
      "         [  0.0000,   0.0000],\n",
      "         [126.5869, 240.8156],\n",
      "         [123.2780, 255.7308],\n",
      "         [121.4662, 256.8020],\n",
      "         [125.8314, 293.9740],\n",
      "         [120.5660, 294.5702],\n",
      "         [122.8581, 327.9831],\n",
      "         [116.3577, 328.9588]],\n",
      "\n",
      "        [[344.0865, 240.8536],\n",
      "         [346.1497, 238.7902],\n",
      "         [342.0577, 238.7465],\n",
      "         [  0.0000,   0.0000],\n",
      "         [339.0043, 240.1344],\n",
      "         [352.5620, 251.9000],\n",
      "         [334.9610, 251.8980],\n",
      "         [  0.0000,   0.0000],\n",
      "         [330.0306, 266.9539],\n",
      "         [  0.0000,   0.0000],\n",
      "         [327.4621, 280.1948],\n",
      "         [349.4850, 280.7694],\n",
      "         [338.2119, 280.7281],\n",
      "         [348.9787, 303.9976],\n",
      "         [338.4152, 303.5789],\n",
      "         [348.3111, 324.7339],\n",
      "         [338.7798, 324.5371]],\n",
      "\n",
      "        [[281.3108, 240.5744],\n",
      "         [282.7548, 238.6565],\n",
      "         [279.1795, 238.5735],\n",
      "         [  0.0000,   0.0000],\n",
      "         [274.8722, 239.5618],\n",
      "         [286.2578, 250.8042],\n",
      "         [268.7912, 249.7148],\n",
      "         [  0.0000,   0.0000],\n",
      "         [262.0956, 259.9972],\n",
      "         [  0.0000,   0.0000],\n",
      "         [265.7159, 264.2663],\n",
      "         [283.4543, 278.3653],\n",
      "         [272.3520, 278.1386],\n",
      "         [281.8365, 301.5149],\n",
      "         [272.3753, 301.0013],\n",
      "         [278.8859, 322.4161],\n",
      "         [270.9407, 322.2700]]])\n",
      "xyn: tensor([[[0.6837, 0.4045],\n",
      "         [0.6888, 0.4010],\n",
      "         [0.6833, 0.4006],\n",
      "         [0.7001, 0.4116],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.7133, 0.4522],\n",
      "         [0.6725, 0.4502],\n",
      "         [0.7165, 0.5029],\n",
      "         [0.6651, 0.5009],\n",
      "         [0.7205, 0.5487],\n",
      "         [0.6575, 0.5442],\n",
      "         [0.7082, 0.5460],\n",
      "         [0.6816, 0.5452],\n",
      "         [0.7067, 0.6211],\n",
      "         [0.6875, 0.6189],\n",
      "         [0.6990, 0.6828],\n",
      "         [0.6903, 0.6916]],\n",
      "\n",
      "        [[0.8435, 0.3710],\n",
      "         [0.8482, 0.3642],\n",
      "         [0.8387, 0.3651],\n",
      "         [0.8561, 0.3703],\n",
      "         [0.8329, 0.3719],\n",
      "         [0.8718, 0.4167],\n",
      "         [0.8238, 0.4167],\n",
      "         [0.8833, 0.4724],\n",
      "         [0.8179, 0.4572],\n",
      "         [0.8901, 0.5203],\n",
      "         [0.8350, 0.4217],\n",
      "         [0.8700, 0.5240],\n",
      "         [0.8373, 0.5257],\n",
      "         [0.8769, 0.6100],\n",
      "         [0.8387, 0.6120],\n",
      "         [0.8842, 0.6853],\n",
      "         [0.8395, 0.6934]],\n",
      "\n",
      "        [[0.2550, 0.2610],\n",
      "         [0.2637, 0.2470],\n",
      "         [0.2481, 0.2464],\n",
      "         [0.2815, 0.2370],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3045, 0.3047],\n",
      "         [0.2199, 0.2996],\n",
      "         [0.3204, 0.4005],\n",
      "         [0.1902, 0.3803],\n",
      "         [0.2784, 0.3941],\n",
      "         [0.2163, 0.3876],\n",
      "         [0.2783, 0.4974],\n",
      "         [0.2253, 0.4938],\n",
      "         [0.2754, 0.6392],\n",
      "         [0.2230, 0.6322],\n",
      "         [0.2657, 0.7742],\n",
      "         [0.2434, 0.7451]],\n",
      "\n",
      "        [[0.4971, 0.4830],\n",
      "         [0.5006, 0.4773],\n",
      "         [0.4939, 0.4775],\n",
      "         [0.5072, 0.4780],\n",
      "         [0.4901, 0.4785],\n",
      "         [0.5151, 0.5057],\n",
      "         [0.4855, 0.5040],\n",
      "         [0.5190, 0.5366],\n",
      "         [0.4785, 0.5296],\n",
      "         [0.5050, 0.5112],\n",
      "         [0.4873, 0.5007],\n",
      "         [0.5063, 0.5740],\n",
      "         [0.4869, 0.5729],\n",
      "         [0.5045, 0.6299],\n",
      "         [0.4874, 0.6281],\n",
      "         [0.5024, 0.6829],\n",
      "         [0.4888, 0.6811]],\n",
      "\n",
      "        [[0.4106, 0.4779],\n",
      "         [0.4134, 0.4733],\n",
      "         [0.4072, 0.4739],\n",
      "         [0.4178, 0.4766],\n",
      "         [0.4027, 0.4778],\n",
      "         [0.4244, 0.5044],\n",
      "         [0.3978, 0.5016],\n",
      "         [0.4306, 0.5351],\n",
      "         [0.3905, 0.5189],\n",
      "         [0.4253, 0.5404],\n",
      "         [0.3958, 0.5039],\n",
      "         [0.4185, 0.5696],\n",
      "         [0.4013, 0.5686],\n",
      "         [0.4171, 0.6243],\n",
      "         [0.4017, 0.6233],\n",
      "         [0.4161, 0.6760],\n",
      "         [0.4067, 0.6743]],\n",
      "\n",
      "        [[0.5861, 0.4983],\n",
      "         [0.5892, 0.4947],\n",
      "         [0.5843, 0.4940],\n",
      "         [0.5945, 0.4974],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.5999, 0.5218],\n",
      "         [0.5755, 0.5199],\n",
      "         [0.6029, 0.5511],\n",
      "         [0.5691, 0.5490],\n",
      "         [0.6001, 0.5747],\n",
      "         [0.5649, 0.5769],\n",
      "         [0.5934, 0.5813],\n",
      "         [0.5781, 0.5803],\n",
      "         [0.5925, 0.6244],\n",
      "         [0.5788, 0.6229],\n",
      "         [0.5917, 0.6646],\n",
      "         [0.5830, 0.6629]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3474, 0.4396],\n",
      "         [0.3055, 0.4381],\n",
      "         [0.3570, 0.4870],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3515, 0.5248],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3355, 0.5475],\n",
      "         [0.3076, 0.5468],\n",
      "         [0.3333, 0.6250],\n",
      "         [0.3034, 0.6242],\n",
      "         [0.3287, 0.6988],\n",
      "         [0.3028, 0.6973]],\n",
      "\n",
      "        [[0.2315, 0.2719],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2248, 0.2617],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2127, 0.2664],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1968, 0.3278],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1725, 0.4004],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2038, 0.4023],\n",
      "         [0.2453, 0.4853],\n",
      "         [0.2092, 0.4852],\n",
      "         [0.2402, 0.6095],\n",
      "         [0.2145, 0.6091],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2252, 0.7284]],\n",
      "\n",
      "        [[0.6241, 0.4957],\n",
      "         [0.6261, 0.4920],\n",
      "         [0.6214, 0.4915],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.6155, 0.4942],\n",
      "         [0.6309, 0.5183],\n",
      "         [0.6086, 0.5181],\n",
      "         [0.6360, 0.5460],\n",
      "         [0.6034, 0.5466],\n",
      "         [0.6301, 0.5488],\n",
      "         [0.6099, 0.5447],\n",
      "         [0.6275, 0.5742],\n",
      "         [0.6122, 0.5749],\n",
      "         [0.6325, 0.6208],\n",
      "         [0.6141, 0.6223],\n",
      "         [0.6382, 0.6651],\n",
      "         [0.6145, 0.6676]],\n",
      "\n",
      "        [[0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1616, 0.4510],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1615, 0.4890],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1739, 0.4965],\n",
      "         [0.1693, 0.5273],\n",
      "         [0.1668, 0.5295],\n",
      "         [0.1728, 0.6061],\n",
      "         [0.1656, 0.6074],\n",
      "         [0.1688, 0.6763],\n",
      "         [0.1598, 0.6783]],\n",
      "\n",
      "        [[0.4726, 0.4966],\n",
      "         [0.4755, 0.4924],\n",
      "         [0.4699, 0.4923],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4657, 0.4951],\n",
      "         [0.4843, 0.5194],\n",
      "         [0.4601, 0.5194],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4533, 0.5504],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.4498, 0.5777],\n",
      "         [0.4801, 0.5789],\n",
      "         [0.4646, 0.5788],\n",
      "         [0.4794, 0.6268],\n",
      "         [0.4649, 0.6259],\n",
      "         [0.4784, 0.6696],\n",
      "         [0.4654, 0.6691]],\n",
      "\n",
      "        [[0.3864, 0.4960],\n",
      "         [0.3884, 0.4921],\n",
      "         [0.3835, 0.4919],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3776, 0.4939],\n",
      "         [0.3932, 0.5171],\n",
      "         [0.3692, 0.5149],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3600, 0.5361],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3650, 0.5449],\n",
      "         [0.3894, 0.5739],\n",
      "         [0.3741, 0.5735],\n",
      "         [0.3871, 0.6217],\n",
      "         [0.3741, 0.6206],\n",
      "         [0.3831, 0.6648],\n",
      "         [0.3722, 0.6645]]])\n"
     ]
    }
   ],
   "source": [
    "image_name = 'images/prueba3.jpg'\n",
    "image = cv.imread(image_name)\n",
    "results = model(image)\n",
    "\n",
    "keypoints = results[0].keypoints\n",
    "\n",
    "print(keypoints)\n",
    "\n",
    "for keypoint in keypoints:\n",
    "    keypoint_coordinates = keypoint.xy[0]\n",
    "    right_wrists = keypoint_coordinates[10]\n",
    "    conf = keypoint.conf[0, 10]\n",
    "\n",
    "    if conf > 0.5:\n",
    "        cv.circle(image, (int(right_wrists[0]), int(right_wrists[1])), 5, (0, 255, 0), -1)\n",
    "    \n",
    "cv.imshow('Output', image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f385ed-b8a9-4848-9293-97e37e93b0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Procesar imágenes de un vídeo y guardar a disco\n",
    "import cv2 as cv\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "video = cv.VideoCapture(\"videos/new_york.mp4\")\n",
    "\n",
    "status, img = video.read()\n",
    "size=(img.shape[1], img.shape[0])\n",
    "\n",
    "fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
    "video_out = cv.VideoWriter('video1.mp4', fourcc, 30.0, size)\n",
    "\n",
    "while status:\n",
    "    orig = img.copy()\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    image = np.expand_dims(img, axis=0)\n",
    "    image = image / 255.0\n",
    "    image = torch.FloatTensor(image).to(device)\n",
    "\n",
    "    detections = model(image)[0]\n",
    "    output = draw_bbox(detections, orig)\n",
    "    \n",
    "    cv.imshow('Salida', output)\n",
    "    video_out.write(output)\n",
    "    \n",
    "    status, img = video.read()\n",
    "    if cv.waitKey(1)==27:\n",
    "        status = False\n",
    "\n",
    "    \n",
    "video.release()\n",
    "video_out.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea2af7-8dc4-401f-903a-3d305cdbaaad",
   "metadata": {},
   "source": [
    "# Seguimiento de Objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3942c90a-a823-4a80-bc6f-e0bf4d047384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Num classes: 91\n"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# esto es para que funcione matplotlib \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# definición de constantes \n",
    "CLASSES = ['__background__', 'person', 'bicycle', 'car', 'motorcycle', \n",
    "           'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "           'fire hydrant', 'N/A', 'stop sign', 'parking meter', 'bench', \n",
    "           'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', \n",
    "           'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', \n",
    "           'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', \n",
    "           'snowboard', 'sports ball', 'kite', 'baseball bat', \n",
    "           'baseball glove', 'skateboard', 'surfboard', 'tennis racket', \n",
    "           'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', \n",
    "           'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', \n",
    "           'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "           'potted plant', 'bed', 'N/A', 'dining table', 'N/A', 'N/A', \n",
    "           'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "           'cell phone', 'microwave', 'oven', 'toaster', 'sink', \n",
    "           'refrigerator', 'N/A', 'book', 'clock', 'vase', 'scissors', \n",
    "           'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "print('Num classes:', len(CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e84ce74c-eea2-47e8-807e-21de0102d2b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculamos la media\n",
    "def compute_means(detections):\n",
    "    new_means = []\n",
    "    new_labels = []\n",
    "    new_bbox = []\n",
    "    for i in range(len(detections[\"boxes\"])):\n",
    "        confidence = detections[\"scores\"][i]\n",
    "        if confidence > 0.8:\n",
    "            box = detections[\"boxes\"][i].detach().cpu().numpy()\n",
    "            x0, y0, x1, y1 = box.astype(\"int\")\n",
    "            new_means.append(((x0+x1)/2,(y0+y1)/2))\n",
    "            new_bbox.append([x0,y0,x1,y1])\n",
    "            \n",
    "            # extraemos la etiqueta\n",
    "            label = int(detections[\"labels\"][i])\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_means, new_labels, new_bbox\n",
    "\n",
    "def distance(a,b):\n",
    "    d1 = a[0]-b[0]\n",
    "    d2 = a[1]-b[1]\n",
    "    return d1*d1+d2*d2\n",
    "\n",
    "def match_objects(means, labels, bbox, new_means, new_labels, new_bbox):\n",
    "    points = []\n",
    "    point_labels = []\n",
    "    for i in range(len(new_means)):\n",
    "        mean = new_means[i]\n",
    "        label = new_labels[i]\n",
    "        box = new_bbox[i]\n",
    "        \n",
    "        j=0\n",
    "        min_dist=9999\n",
    "        min_pos=9999\n",
    "        threshold = 200\n",
    "        while j<len(labels):\n",
    "            d = distance(mean, means[j][-1])\n",
    "            if label == labels[j] and d<min_dist:\n",
    "                min_dist = d\n",
    "                min_pos = j\n",
    "            j+=1\n",
    "        \n",
    "        if min_dist>threshold:\n",
    "            means.append([mean])\n",
    "            labels.append(label)\n",
    "            bbox.append([box])\n",
    "        else:\n",
    "            means[min_pos].append(mean)\n",
    "            bbox[min_pos].append(box)\n",
    "            points.append(mean)\n",
    "            point_labels.append(label)\n",
    "            \n",
    "    return points, point_labels\n",
    "    \n",
    "def draw_points(tracking, points, point_labels):\n",
    "    for i in range(len(points)):\n",
    "        color = (255,0,0) #COLORS[point_labels[i]] #(255,0,0)\n",
    "        tracking[int(points[i][1])-1, int(points[i][0])-1]=color\n",
    "        tracking[int(points[i][1])-1, int(points[i][0])]=color\n",
    "        tracking[int(points[i][1]), int(points[i][0])-1]=color\n",
    "        tracking[int(points[i][1]), int(points[i][0])]=color\n",
    "        tracking[int(points[i][1])+1, int(points[i][0])]=color\n",
    "        tracking[int(points[i][1]), int(points[i][0])+1]=color\n",
    "        tracking[int(points[i][1])+1, int(points[i][0])+1]=color\n",
    "        tracking[int(points[i][1])-1, int(points[i][0])+1]=color\n",
    "        tracking[int(points[i][1])+1, int(points[i][0])-1]=color\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548de0ca-62e6-4d76-afbb-ae48e6b27e15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "targets should not be none when in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m image \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     36\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 37\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# calculamos la media\u001b[39;00m\n\u001b[0;32m     40\u001b[0m new_means, new_labels, new_bbox \u001b[38;5;241m=\u001b[39m compute_means(detections)\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:62\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n",
      "File \u001b[1;32mc:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torch\\__init__.py:1404\u001b[0m, in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[1;32m-> 1404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[1;31mAssertionError\u001b[0m: targets should not be none when in training mode"
     ]
    }
   ],
   "source": [
    "# Procesar imágenes de un vídeo o de la webcam\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.models import detection\n",
    "\n",
    "model = detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True).to(device)\n",
    "\n",
    "#video = cv.VideoCapture(0)\n",
    "video = cv.VideoCapture(\"videos/people2.mp4\")\n",
    "\n",
    "\n",
    "status, img = video.read()\n",
    "size=(img.shape[1], img.shape[0])\n",
    "\n",
    "fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
    "video_out = cv.VideoWriter('video1.mp4', fourcc, 20.0, size)\n",
    "\n",
    "tracking = 255*np.ones((img.shape[0], img.shape[1], 3))\n",
    "\n",
    "model = YOLO('./yolov8x-pose.pt')\n",
    "\n",
    "means = []\n",
    "labels = []\n",
    "bbox = []\n",
    "pos = 10\n",
    "while status:\n",
    "    \n",
    "    # adaptamos la nueva imagen\n",
    "    orig = img.copy()\n",
    "    original = img.copy()\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    # calculamos las predicciones\n",
    "    image = np.expand_dims(img, axis=0)\n",
    "    image = image / 255.0\n",
    "    image = torch.FloatTensor(image).to(device)\n",
    "    detections = model(image)\n",
    "\n",
    "    keypoints = detections[0].keypoints\n",
    "\n",
    "    for keypoint in keypoints:\n",
    "        keypoint_coordinates = keypoint.xy[0]\n",
    "        right_wrists = keypoint_coordinates[10]\n",
    "        conf = keypoint.conf[0, 10]\n",
    "\n",
    "        if conf > 0.5:\n",
    "            cv.circle(image, (int(right_wrists[0]), int(right_wrists[1])), 5, (0, 255, 0), -1)\n",
    "        \n",
    "    cv.imshow('Output', image)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    # calculamos la media\n",
    "    new_means, new_labels, new_bbox = compute_means(detections)\n",
    "    \n",
    "    # buscamos la correspondencia anterior\n",
    "    points, point_labels = match_objects(means, labels, bbox, new_means, new_labels, new_bbox)\n",
    "    \n",
    "    # dibujamos los nuevos puntos de las trayectorias\n",
    "    draw_points(tracking, points, point_labels)\n",
    "    \n",
    "    # dibujamos los objetos detectados\n",
    "    img_detections = draw_bbox(detections, orig, labels)\n",
    "    \n",
    "    # mezclamos las dos imágenes\n",
    "    output = img_detections&tracking.astype(np.uint8)\n",
    "    output[output>255]=255\n",
    "    output[output<0]=0\n",
    "    \n",
    "    # mostramos el contenido de un bounding box aparte\n",
    "    if len(bbox)>pos:\n",
    "        x0=bbox[pos][-1][0]\n",
    "        y0=bbox[pos][-1][1]\n",
    "        x1=bbox[pos][-1][2]\n",
    "        y1=bbox[pos][-1][3]\n",
    "\n",
    "        if x0<x1 and y0<y1:\n",
    "            subimage=original[y0:y1,x0:x1,:]\n",
    "            dim = (3*subimage.shape[1], 3*subimage.shape[0])\n",
    "            subimage=cv.resize(subimage,dim, interpolation = cv.INTER_AREA)\n",
    "            cv.imshow('Bbox', subimage)\n",
    "            \n",
    "    cv.imshow('Output', output.astype(np.uint8))\n",
    "    cv.imshow('Tracking', tracking)\n",
    "    \n",
    "    video_out.write(output)\n",
    "    \n",
    "    status, img = video.read()\n",
    "    \n",
    "    if cv.waitKey(1)==27:\n",
    "        status = False\n",
    "    \n",
    "video.release()\n",
    "video_out.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "313e397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandPerson():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "    \n",
    "    def add(self, x, y):\n",
    "        self.x.append(x)\n",
    "        self.y.append(y)\n",
    "    \n",
    "    def calculate_distance(self, x, y):\n",
    "        if len(self.x)>0 and len(self.y)>0:\n",
    "            distance = np.sqrt((x-self.x[-1])**2 + (y-self.y[-1])**2)\n",
    "            if distance<5:\n",
    "                self.add(x,y)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            self.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "242bf2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gerar\\anaconda3\\envs\\aa1\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 11 persons, 971.1ms\n",
      "Speed: 4.2ms preprocess, 971.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1006.6ms\n",
      "Speed: 2.5ms preprocess, 1006.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 992.5ms\n",
      "Speed: 8.5ms preprocess, 992.5ms inference, 10.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 983.3ms\n",
      "Speed: 0.0ms preprocess, 983.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 980.3ms\n",
      "Speed: 0.0ms preprocess, 980.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 981.1ms\n",
      "Speed: 3.0ms preprocess, 981.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 982.0ms\n",
      "Speed: 3.0ms preprocess, 982.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 969.3ms\n",
      "Speed: 3.5ms preprocess, 969.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 977.0ms\n",
      "Speed: 3.0ms preprocess, 977.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Procesar imágenes de un vídeo o de la webcam\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.models import detection\n",
    "\n",
    "model = detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True).to(device)\n",
    "\n",
    "#video = cv.VideoCapture(0)\n",
    "video = cv.VideoCapture(\"videos/people2.mp4\")\n",
    "\n",
    "\n",
    "status, img = video.read()\n",
    "size=(img.shape[1], img.shape[0])\n",
    "\n",
    "fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
    "video_out = cv.VideoWriter('video1.mp4', fourcc, 20.0, size)\n",
    "\n",
    "tracking = 255*np.ones((img.shape[0], img.shape[1], 3))\n",
    "\n",
    "model = YOLO('./yolov8x-pose.pt')\n",
    "\n",
    "person = HandPerson()\n",
    "coordinate_x = []\n",
    "coordinate_y = []\n",
    "pos = 10\n",
    "t = 0\n",
    "time = []\n",
    "i = 0\n",
    "while status:\n",
    "    \n",
    "    # adaptamos la nueva imagen\n",
    "    orig = img.copy()\n",
    "    original = img.copy()\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    detections = model(img)\n",
    "\n",
    "    if i < 1:\n",
    "        keypoints = detections[0].keypoints\n",
    "        keypoint = keypoints[5]\n",
    "        keypoint_coordinates = keypoint.xy[0]\n",
    "        right_wrists = keypoint_coordinates[10]\n",
    "        person.add(int(right_wrists[0]), int(right_wrists[1]))\n",
    "    else:\n",
    "        keypoints = detections[0].keypoints\n",
    "\n",
    "        for keypoint in keypoints:\n",
    "            keypoint_coordinates = keypoint.xy[0]\n",
    "            right_wrists = keypoint_coordinates[10]\n",
    "            conf = keypoint.conf[0, 10]\n",
    "            if conf > 0.2:\n",
    "                cv.circle(img, (int(right_wrists[0]), int(right_wrists[1])), 5, (0, 255, 0), -1)\n",
    "                person.calculate_distance(int(right_wrists[0]), int(right_wrists[1]))\n",
    "    \n",
    "    time.append(i)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    cv.imshow('Tracking', img)\n",
    "    \n",
    "    status, img = video.read()\n",
    "    \n",
    "    if cv.waitKey(1)==27:\n",
    "        status = False\n",
    "    \n",
    "video.release()\n",
    "video_out.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7ac3923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n",
      "[543, 545, 545, 544, 544, 545] [353, 350, 346, 342, 338, 335]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = [i for i in range(len(person.x))]\n",
    "\n",
    "print(t)\n",
    "print(person.x, person.y)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(t, person.x)\n",
    "plt.plot(t, person.y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ace64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar imágenes de un vídeo o de la webcam\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.models import detection\n",
    "\n",
    "model = detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "#video = cv.VideoCapture(0)\n",
    "video = cv.VideoCapture(\"videos/people2.mp4\")\n",
    "\n",
    "\n",
    "status, img = video.read()\n",
    "size=(img.shape[1], img.shape[0])\n",
    "\n",
    "fourcc = cv.VideoWriter_fourcc(*'DIVX')\n",
    "video_out = cv.VideoWriter('video1.mp4', fourcc, 20.0, size)\n",
    "\n",
    "tracking = 255*np.ones((img.shape[0], img.shape[1], 3))\n",
    "\n",
    "means = []\n",
    "labels = []\n",
    "bbox = []\n",
    "pos = 10\n",
    "while status:\n",
    "    \n",
    "    # adaptamos la nueva imagen\n",
    "    orig = img.copy()\n",
    "    original = img.copy()\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    # calculamos las predicciones\n",
    "    image = np.expand_dims(img, axis=0)\n",
    "    image = image / 255.0\n",
    "    image = torch.FloatTensor(image).to(device)\n",
    "    detections = model(image)[0]\n",
    "    \n",
    "    # calculamos la media\n",
    "    new_means, new_labels, new_bbox = compute_means(detections)\n",
    "    \n",
    "    # buscamos la correspondencia anterior\n",
    "    points, point_labels = match_objects(means, labels, bbox, new_means, new_labels, new_bbox)\n",
    "    \n",
    "    # dibujamos los nuevos puntos de las trayectorias\n",
    "    draw_points(tracking, points, point_labels)\n",
    "    \n",
    "    # dibujamos los objetos detectados\n",
    "    img_detections = draw_bbox(detections, orig, labels)\n",
    "    \n",
    "    # mezclamos las dos imágenes\n",
    "    output = img_detections&tracking.astype(np.uint8)\n",
    "    output[output>255]=255\n",
    "    output[output<0]=0\n",
    "    \n",
    "    # mostramos el contenido de un bounding box aparte\n",
    "    if len(bbox)>pos:\n",
    "        x0=bbox[pos][-1][0]\n",
    "        y0=bbox[pos][-1][1]\n",
    "        x1=bbox[pos][-1][2]\n",
    "        y1=bbox[pos][-1][3]\n",
    "\n",
    "        if x0<x1 and y0<y1:\n",
    "            subimage=original[y0:y1,x0:x1,:]\n",
    "            dim = (3*subimage.shape[1], 3*subimage.shape[0])\n",
    "            subimage=cv.resize(subimage,dim, interpolation = cv.INTER_AREA)\n",
    "            cv.imshow('Bbox', subimage)\n",
    "            \n",
    "    cv.imshow('Output', output.astype(np.uint8))\n",
    "    cv.imshow('Tracking', tracking)\n",
    "    \n",
    "    video_out.write(output)\n",
    "    \n",
    "    status, img = video.read()\n",
    "    \n",
    "    if cv.waitKey(1)==27:\n",
    "        status = False\n",
    "    \n",
    "video.release()\n",
    "video_out.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda9e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
