{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando SEA-Raft para oprical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflow_viz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flow_to_image\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mraft_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RAFT\n",
      "File \u001b[1;32mc:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PIAV\\Pract\\P07\\deeplearning\\raft_model.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mupdate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasicUpdateBlock\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcorr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CorrBlock\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coords_grid, InputPadder\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mextractor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNetFPN\n",
      "File \u001b[1;32mc:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PIAV\\Pract\\P07\\deeplearning\\corr.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coords_grid, bilinear_sampler\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malt_cuda_corr\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from flow_viz import flow_to_image\n",
    "from raft_model import RAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susana\\AppData\\Local\\Temp\\ipykernel_12900\\3807378895.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'load_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m(state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'load_state_dict'"
     ]
    }
   ],
   "source": [
    "model_path = \".\\models\\Tartan-C-T-TSKH-spring540x960-M.pth\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Susana\\anaconda3\\envs\\piav\\lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_video\n",
    "frames, _, _ = read_video(\"../videos/people2.mp4\")\n",
    "frames = frames.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "img1_batch = torch.stack([frames[100], frames[150]])\n",
    "img2_batch = torch.stack([frames[101], frames[151]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Susana\\AppData\\Local\\Temp\\ipykernel_12900\\1354343357.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image1 = torch.tensor(frames[100], dtype=torch.float32).permute(2, 0, 1)\n",
      "C:\\Users\\Susana\\AppData\\Local\\Temp\\ipykernel_12900\\1354343357.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image2 = torch.tensor(frames[150], dtype=torch.float32).permute(2, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "image1 = torch.tensor(frames[100], dtype=torch.float32).permute(2, 0, 1)\n",
    "image2 = torch.tensor(frames[150], dtype=torch.float32).permute(2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_flow(args, model, image1, image2):\n",
    "    output = model(image1, image2, iters=args.iters, test_mode=True)\n",
    "    flow_final = output['flow'][-1]\n",
    "    info_final = output['info'][-1]\n",
    "    return flow_final, info_final\n",
    "\n",
    "def calc_flow(args, model, image1, image2):\n",
    "    img1 = F.interpolate(image1, scale_factor=2 ** args.scale, mode='bilinear', align_corners=False)\n",
    "    img2 = F.interpolate(image2, scale_factor=2 ** args.scale, mode='bilinear', align_corners=False)\n",
    "    H, W = img1.shape[2:]\n",
    "    flow, info = forward_flow(args, model, img1, img2)\n",
    "    flow_down = F.interpolate(flow, scale_factor=0.5 ** args.scale, mode='bilinear', align_corners=False) * (0.5 ** args.scale)\n",
    "    info_down = F.interpolate(info, scale_factor=0.5 ** args.scale, mode='area')\n",
    "    return flow_down, info_down\n",
    "\n",
    "def get_heatmap(info, args):\n",
    "    raw_b = info[:, 2:]\n",
    "    log_b = torch.zeros_like(raw_b)\n",
    "    weight = info[:, :2].softmax(dim=1)              \n",
    "    log_b[:, 0] = torch.clamp(raw_b[:, 0], min=0, max=args.var_max)\n",
    "    log_b[:, 1] = torch.clamp(raw_b[:, 1], min=args.var_min, max=0)\n",
    "    heatmap = (log_b * weight).sum(dim=1, keepdim=True)\n",
    "    return heatmap\n",
    "\n",
    "def create_color_bar(height, width, color_map):\n",
    "    \"\"\"\n",
    "    Create a color bar image using a specified color map.\n",
    "\n",
    "    :param height: The height of the color bar.\n",
    "    :param width: The width of the color bar.\n",
    "    :param color_map: The OpenCV colormap to use.\n",
    "    :return: A color bar image.\n",
    "    \"\"\"\n",
    "    # Generate a linear gradient\n",
    "    gradient = np.linspace(0, 255, width, dtype=np.uint8)\n",
    "    gradient = np.repeat(gradient[np.newaxis, :], height, axis=0)\n",
    "\n",
    "    # Apply the colormap\n",
    "    color_bar = cv.applyColorMap(gradient, color_map)\n",
    "\n",
    "    return color_bar\n",
    "\n",
    "def add_color_bar_to_image(image, color_bar, orientation='vertical'):\n",
    "    \"\"\"\n",
    "    Add a color bar to an image.\n",
    "\n",
    "    :param image: The original image.\n",
    "    :param color_bar: The color bar to add.\n",
    "    :param orientation: 'vertical' or 'horizontal'.\n",
    "    :return: Combined image with the color bar.\n",
    "    \"\"\"\n",
    "    if orientation == 'vertical':\n",
    "        return cv.vconcat([image, color_bar])\n",
    "    else:\n",
    "        return cv.hconcat([image, color_bar])\n",
    "\n",
    "def vis_heatmap(name, image, heatmap):\n",
    "    # theta = 0.01\n",
    "    # print(heatmap.max(), heatmap.min(), heatmap.mean())\n",
    "    heatmap = heatmap[:, :, 0]\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "    # heatmap = heatmap > 0.01\n",
    "    heatmap = (heatmap * 255).astype(np.uint8)\n",
    "    colored_heatmap = cv.applyColorMap(heatmap, cv.COLORMAP_JET)\n",
    "    overlay = image * 0.3 + colored_heatmap * 0.7\n",
    "    # Create a color bar\n",
    "    height, width = image.shape[:2]\n",
    "    color_bar = create_color_bar(50, width, cv.COLORMAP_JET)  # Adjust the height and colormap as needed\n",
    "    # Add the color bar to the image\n",
    "    overlay = overlay.astype(np.uint8)\n",
    "    combined_image = add_color_bar_to_image(overlay, color_bar, 'vertical')\n",
    "    cv.imwrite(name, cv.cvtColor(combined_image, cv.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = image1.shape[1:]\n",
    "image1 = image1[None].to(device)\n",
    "image2 = image2[None].to(device)\n",
    "\n",
    "H, W = image1.shape[2:]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cfg', help='experiment configure file name', required=True, type=str)\n",
    "parser.add_argument('--path', help='checkpoint path', type=str, default=None)\n",
    "parser.add_argument('--url', help='checkpoint url', type=str, default=None)\n",
    "parser.add_argument('--device', help='inference device', type=str, default='cpu')\n",
    "args = parse_args(parser)\n",
    "\n",
    "flow, info = calc_flow(args, model, image1, image2)\n",
    "flow_vis = flow_to_image(flow[0].permute(1, 2, 0).cpu().numpy(), convert_to_bgr=True)\n",
    "cv.imwrite(f\"./flow.jpg\", flow_vis)\n",
    "heatmap = get_heatmap(info, args)\n",
    "vis_heatmap(f\"./heatmap.jpg\", image1[0].permute(1, 2, 0).cpu().numpy(), heatmap[0].permute(1, 2, 0).cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
